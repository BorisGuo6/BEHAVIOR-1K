import json
import math
import os
import random
import subprocess
from dask.distributed import Client, as_completed
import fs.copy
from fs.multifs import MultiFS
import fs.path
from fs.tempfs import TempFS
import tqdm

from b1k_pipeline.utils import ParallelZipFS, PipelineFS, TMP_DIR

WORKER_COUNT = 4
BATCH_SIZE = 1
RETRY = False

ids = {
    "dhkkfo",
    "nfuxzd",
    "lgopij",
    "yhurut",
    "gfxrnj",
    "swytaw",
    "dtjmai",
    "effbnc",
    "exzsal",
    "fxrsyi",
    "fyrkzs",
    "jdijek",
    "qwoqqr",
    "rhohgs",
    "sfvswx",
    "sstojv",
    "uobdoq",
    "uzgibd",
    "zycgen",
    "ecqxgd",
    "nigfha",
    "lymciz",
    "mxsliu",
    "rxscji",
    "xtqbuf",
    "dpxnlc",
    "hvlkjx",
    "cwkvib",
    "dhfqid",
    "eyedvd",
    "xnjqix",
    "adciys",
    "ajzltc",
    "aspeds",
    "belcml",
    "bexgtn",
    "bnobdx",
    "byzaxy",
    "ckxwea",
    "cypjlv",
    "dalyim",
    "eawgwj",
    "eipwho",
    "fedafr",
    "feuaak",
    "fiarri",
    "fwdfeg",
    "hitnkv",
    "hpqjug",
    "hynhgz",
    "jblalf",
    "jfvjep",
    "jhtxxh",
    "jpvcjv",
    "kasebx",
    "kdkrov",
    "kthvrl",
    "lgaxzt",
    "mspdar",
    "nkkhbn",
    "npuuir",
    "oyidja",
    "pihjqa",
    "qzodht",
    "rbnyxi",
    "rlwpcd",
    "sqqahm",
    "szgdpc",
    "tvtive",
    "tyczoo",
    "vccsrl",
    "wryghu",
    "wtepsx",
    "xplzbo",
    "xpnlup",
    "pbvpua",
    "uftzyo",
    "bdhvnt",
    "lsmlzi",
    "wlilma",
    "qixpto",
    "gqwnfv",
    "xcppkc",
    "ouhqnw",
    "ttxunv",
    "hdbsog",
    "mdtkkv",
    "ocjcgp",
    "causya",
    "cdmmwy",
    "hhlmbi",
    "libote",
    "msfzpz",
    "sxlklf",
    "uartvl",
    "ylrxhe",
    "mmbavt",
    "ncbsee",
    "heuzgu",
    "rclizj",
    "qfvqfm",
    "lbcxwi",
    "zndohl",
    "sfkezf",
    "xkixrg",
    "ztripg",
    "ooyqcr",
    "jeqtzg",
    "cvdbum",
    "gklybu",
    "hacehh",
    "jwxbpa",
    "qhnpmc",
    "qlxhhh",
    "uzkxtz",
    "yegrkf",
    "nsxhvs",
    "ovjhuf",
    "cqdioi",
    "xevdnl",
    "kxwgoo",
    "cjmtvq",
    "phimqa",
    "loduxu",
    "ckkwmj",
    "dkxddg",
    "fgizgn",
    "ibhhfj",
    "nbhcgu",
    "nhzrei",
    "rixzrk",
    "rypdvd",
    "siksnl",
    "skamgp",
    "xjdyon",
    "ykuftq",
    "oyqdtz",
    "fvkdos",
    "cfdond",
    "gqemcq",
    "dhseui",
    "lfjmos",
    "hfclfn",
    "lspxjq",
    "xdahvv",
    "ofasfw",
    "yufawg",
    "ifqdxn",
    "vxxcvg",
    "xmxvml",
    "qmdgct",
    "hkdsla",
    "ceaeqf",
    "qornxa",
    "bpwjxr",
    "iwwpsf",
    "uyixwc",
    "qxnzpx",
    "fzhcdb",
    "zhsjcs",
    "gamkbo",
    "ngcvaw",
    "ztyxyi",
    "aewpzn",
    "cprjvq",
    "hzspwg",
    "jpzusm",
    "mhndon",
    "sfbdjn",
    "snvhlz",
    "vycozd",
    "bfaqfe",
    "oxknkz",
    "zdxagk",
    "nawrfs",
    "egpkea",
    "yzeuqo",
    "qwthua",
    "oywwzz",
    "fxnjfr",
    "arryyl",
    "krgqwl",
    "lwsgzd",
    "aeslmf",
    "dhgtvg",
    "qvpthd",
    "nuzkuf",
    "wbnkfk",
    "jvnqly",
    "luhnej",
    "vlurir",
    "lulzdz",
    "gcyvrx",
    "ycgxwb",
    "lkbvad",
    "wengzf",
    "acsllv",
    "bnrvcs",
    "bpxhso",
    "bqpmsv",
    "busiti",
    "crlhmi",
    "dlvall",
    "gejwoi",
    "gkakwk",
    "gqtsam",
    "hjrnct",
    "ifgcmr",
    "iuydyz",
    "jdwvyt",
    "jnjtrl",
    "kfzxah",
    "kijnrj",
    "lvuvbf",
    "mefezc",
    "miivhi",
    "mlnuza",
    "mxhrcl",
    "ociqav",
    "pjaljg",
    "qdnmwg",
    "sjwgfn",
    "vxqpnm",
    "vyfehw",
    "waousd",
    "wcqjew",
    "zdeyzf",
    "nftsal",
    "lrjoro",
    "aysfhf",
    "oqyoos",
    "gjgwvi",
    "hjjeeh",
    "llexze",
    "pvxfot",
    "quzmfw",
    "bzisss",
    "vjbldp",
    "qsdqik",
    "yprkek",
    "bnekjp",
    "rsvypp",
    "ykfkyq",
    "hazvbh",
    "rwnakn",
    "omknho",
    "adxzhe",
    "wigtue",
    "owqbsb",
    "cydfkt",
    "ahtzhp",
    "icvmix",
    "bsgybx",
    "deudkt",
    "xifive",
    "xjzyfc",
    "dhnxww",
    "ehnmxj",
    "fapsrj",
    "jgethp",
    "kewbyf",
    "kitxam",
    "lgxhsc",
    "ntgftr",
    "ppdqbj",
    "ppzttc",
    "waqrdy",
    "yiamah",
    "yxaapv",
    "zsrpiu",
    "lgxfyv",
    "tmjxno",
    "xdhysb",
    "fjpams",
    "gcixra",
    "yoxfyu",
    "zfvhus",
    "jjlfla",
    "bzsxgw",
    "ruryqd",
    "wvhmww",
    "guobeq",
    "jaypjo",
    "xdxqxj",
    "jgyqpd",
    "fhfqys",
    "vbiqcq",
    "xfqatj",
    "csvdbe",
    "wsasmm",
    "barzwx",
    "ankfvi",
    "bbewjo",
    "mbrlge",
    "ompiss",
    "tsyims",
    "tzbnmh",
    "wmkwhg",
    "ihnfbi",
    "skbcqq",
    "vhglly",
    "ygrtaz",
    "aewthq",
    "akfjxx",
    "amhlqh",
    "aynjhg",
    "bgxzec",
    "dbprwc",
    "dnqekb",
    "efkgcw",
    "eixyyn",
    "eozsdg",
    "fhdyrj",
    "fkpaie",
    "haewxp",
    "iawoof",
    "ihrjrb",
    "itoeew",
    "ivbrtz",
    "ivuveo",
    "iwfvwf",
    "kkjiko",
    "kkmkbd",
    "ksgizx",
    "lixwwc",
    "lkomhp",
    "luhkiz",
    "molqhs",
    "mtetqm",
    "nbuspz",
    "nhodax",
    "nikfgd",
    "nmhxfz",
    "nrjump",
    "ntedfx",
    "odmjdd",
    "pjinwe",
    "pkkgzc",
    "pyilfa",
    "qbxfmv",
    "qtfzeq",
    "qyuyjr",
    "spppps",
    "tgrsui",
    "uakqei",
    "ujodgo",
    "uumkbl",
    "vitdwc",
    "vjqzwa",
    "vtjwof",
    "wgcgia",
    "wqgndf",
    "xfjmld",
    "xtdcau",
    "ypdfrp",
    "zpddxu",
    "csanbr",
    "ekjpdj",
    "hnlivs",
    "iadlti",
    "ieoasd",
    "kiiamx",
    "hldhxl",
    "hdcpqg",
    "otyngn",
    "wyojnz",
    "svkdji",
    "yowyst",
    "tnjpsf",
    "trtrsl",
    "uaijua",
    "ukayce",
    "xstykf",
    "duugbb",
    "nuoypc",
    "dafdgk",
    "fjytro",
    "hmzafz",
    "injdmj",
    "tqyiso",
    "ueagnt",
    "ugqdao",
    "dhdhul",
    "kydilb",
    "wdpcmk",
    "fsinsu",
    "chjetk",
    "fbfmwt",
    "kvgaar",
    "obuxbe",
    "ozrwwk",
    "pkfydm",
    "sthkfz",
    "tfzijn",
    "urqzec",
    "uvzmss",
    "vqtkwq",
    "wfryvm",
    "cjsbft",
    "mgbeah",
    "oxivmf",
    "szzjzd",
    "vghfkh",
    "bupgpj",
    "tukaoq",
    "nedrsh",
    "vsxhsv",
    "gswpdr",
    "hamffy",
    "xbkwbi",
    "fsfsas",
    "gnzegv",
    "lpanoc",
    "vicaqs",
    "upfssc",
    "vxtjjn",
    "gzcqwx",
    "nbctrk",
    "saujjl",
    "vlplhs",
    "azoiaq",
    "dcleem",
    "fuzmdd",
    "grrcna",
    "gxiqbw",
    "lfnbhc",
    "oshwps",
    "yvhmex",
    "xixblr",
    "kdlbbq",
    "dhwlaw",
    "kohria",
    "qjhauf",
    "sbvksi",
    "vnvmkx",
    "bsdexp",
    "cpozxi",
    "kccqwj",
    "oxfzfe",
    "tfzfam",
    "vckahe",
    "wopjex",
    "zdvgol",
    "foaehs",
    "jlalfc",
    "mvrhya",
    "apybok",
    "iejmzf",
    "qwtyqj",
    "tgodzn",
    "vnmcfg",
    "ykvekt",
    "iyrrna",
    "krarex",
    "krfzqk",
    "aefcem",
    "cdzyew",
    "cjmezk",
    "djgllo",
    "dnvpag",
    "eahqyq",
    "fkosow",
    "gilsji",
    "glzckq",
    "gsgutn",
    "gvnfgj",
    "gxajos",
    "hqdnjz",
    "hxsyxo",
    "ifzxzj",
    "jlawet",
    "leazin",
    "mcukuh",
    "mdojox",
    "pdmzhv",
    "rbqckd",
    "rteihy",
    "uknjdm",
    "vasiit",
    "wklill",
    "wkxtxh",
    "xkqkbf",
    "zotrbg",
    "avotsj",
    "coqeme",
    "glwebh",
    "gsxbym",
    "hbjdlb",
    "hjxczh",
    "huwhjg",
    "hvlfig",
    "iaaiyi",
    "incirm",
    "jpcflq",
    "mhhoga",
    "mkdcha",
    "mmegts",
    "spopfj",
    "thkphg",
    "tkgsho",
    "txcjux",
    "uekqey",
    "vxbtax",
    "wbwmcs",
    "xzcnjq",
    "yqtlhy",
    "zcmnji",
    "zsddtq",
    "mkstwr",
    "drevku",
    "aegxpb",
    "atgnsc",
    "bbduix",
    "bedkqu",
    "cvyops",
    "dfjcsi",
    "dwspgo",
    "dxnzuk",
    "eqhgiy",
    "euqzpy",
    "gopbrh",
    "hkwtnf",
    "hliauj",
    "htyvuz",
    "icpews",
    "ipbgrw",
    "jdddsr",
    "jpwsrp",
    "kjeudr",
    "mawxva",
    "mdmwcs",
    "meetii",
    "nodcpg",
    "nuqzjs",
    "pqsamn",
    "qebiei",
    "rfegnv",
    "rfigof",
    "rusmlm",
    "rwotxo",
    "saenda",
    "sakwru",
    "stqkvx",
    "szsudo",
    "tjrbxv",
    "toreid",
    "twknia",
    "uuypot",
    "vmbzmm",
    "wltgjn",
    "wmuysk",
    "xfduug",
    "ysdoep",
    "zaziny",
    "zwekzu",
    "hbsbwt",
    "ykysuc",
    "bojwlu",
    "xixlzr",
    "yjmnej",
    "dobgmu",
    "jgyzhv",
    "mrgspe",
    "omeuop",
    "xusefg",
    "ynwamu",
    "zgzvcv",
    "ziomqg",
    "ackxiy",
    "lzdzkk",
    "bbpraa",
    "cdteyb",
    "edfzlt",
    "elwfms",
    "evaida",
    "ewgotr",
    "ggpnlr",
    "gypzlg",
    "igyuko",
    "imsnkt",
    "kttdbu",
    "ktuvuo",
    "kuiiai",
    "lvqgvn",
    "nfoydb",
    "onbiqg",
    "ptciim",
    "qbejli",
    "slscza",
    "szjfpb",
    "uwtdng",
    "vcwsbm",
    "wvztiw",
    "ybhepe",
    "zbridw",
    "msaevo",
    "jpduev",
    "xiwkwz",
    "gtwngf",
    "hlzfxw",
    "vjdkci",
    "zuctnl",
    "vqtevv",
    "aakcyj",
    "adiwil",
    "akusda",
    "bnored",
    "bovcqx",
    "cmdagy",
    "euzudc",
    "exasdr",
    "ezsdil",
    "ggbdlq",
    "hxccge",
    "jzmrdd",
    "kxovsj",
    "oadvet",
    "ovoceo",
    "vxmzmq",
    "yfzibn",
    "pobfpe",
    "vmajcm",
    "ahbhsd",
}


def run_on_batch(dataset_path, batch):
    python_cmd = ["python", "-m", "b1k_pipeline.usd_conversion.generate_fillable_volumes_process", dataset_path] + batch
    cmd = ["conda", "run", "-n", "omnigibson"] + python_cmd
    obj = batch[0][:-1].split("/")[-1]
    with open(f"/scr/ig_pipeline/logs/{obj}.log", "w") as f, open(f"/scr/ig_pipeline/logs/{obj}.err", "w") as ferr:
        return subprocess.run(cmd, stdout=f, stderr=ferr, check=True, cwd="/scr/ig_pipeline")


def main():
    failed_objects = set()
    with PipelineFS() as pipeline_fs, \
         ParallelZipFS("objects_usd.zip") as objects_fs, \
         ParallelZipFS("metadata.zip") as metadata_fs, \
         ParallelZipFS("systems.zip") as systems_fs, \
         TempFS(temp_dir=str(TMP_DIR)) as dataset_fs:
        with ParallelZipFS("fillable_volumes.zip", write=True) as out_fs:
            # Copy everything over to the dataset FS
            print("Copying input to dataset fs...")
            fs.copy.copy_fs(metadata_fs, dataset_fs)
            fs.copy.copy_fs(systems_fs, dataset_fs)
            objdir_glob = list(objects_fs.glob("objects/*/*/"))
            for item in tqdm.tqdm(objdir_glob):
                if fs.path.parts(item.path)[-1] not in ids:
                    continue
                fs.copy.copy_fs(objects_fs.opendir(item.path), dataset_fs.makedirs(item.path))

            print("Launching cluster...")
            dask_client = Client(n_workers=0, host="", scheduler_port=8786)
            # subprocess.run('ssh sc.stanford.edu "cd /cvgl2/u/cgokmen/ig_pipeline/b1k_pipeline/docker; sbatch --parsable run_worker_slurm.sh capri32.stanford.edu:8786"', shell=True, check=True)
            # subprocess.run(f'cd /scr/ig_pipeline/b1k_pipeline/docker; ./run_worker_local.sh {WORKER_COUNT} cgokmen-lambda.stanford.edu:8786', shell=True, check=True)
            print("Waiting for workers")
            dask_client.wait_for_workers(WORKER_COUNT)

            # Start the batched run
            object_glob = [x.path for x in dataset_fs.glob("objects/*/*/")]
            print("Queueing batches.")
            print("Total count: ", len(object_glob))

            # Make sure workers don't idle by reducing batch size when possible.
            batch_size = min(BATCH_SIZE, math.ceil(len(object_glob) / WORKER_COUNT))

            futures = {}
            for start in range(0, len(object_glob), batch_size):
                end = start + batch_size
                batch = object_glob[start:end]
                worker_future = dask_client.submit(
                    run_on_batch,
                    dataset_fs.getsyspath("/"),
                    batch,
                    pure=False)
                futures[worker_future] = batch

            # Wait for all the workers to finish
            print("Queued all batches. Waiting for them to finish...")
            logs = []
            while True:
                for future in tqdm.tqdm(as_completed(futures.keys()), total=len(futures)):
                    # Check the batch results.
                    batch = futures[future]
                    if future.exception():
                        e = future.exception()
                        # logs.append({"stdout": e.stdout.decode("utf-8"), "stderr": e.stderr.decode("utf-8")})
                        print(e)
                    else:
                        out = future.result()
                        # logs.append({"stdout": out.stdout.decode("utf-8"), "stderr": out.stderr.decode("utf-8")})

                    # Remove everything that failed and make a new batch from them.
                    if RETRY:
                      new_batch = []
                      for item in batch:
                          item_dir = dataset_fs.opendir(item)
                          if not item_dir.exists("fillable.obj"):
                              print("Could not find", item)
                              new_batch.append(item)

                      # If there's nothing to requeue, we are good!
                      if not new_batch:
                          continue

                      # Otherwise, decide if we are going to requeue or just skip.
                      if len(batch) == 1:
                          print(f"Failed on a single item {batch[0]}. Skipping.")
                          failed_objects.add(batch[0])
                      else:
                          print(f"Subdividing batch of length {len(new_batch)}")
                          batch_size = len(new_batch) // 2
                          subbatches = [new_batch[:batch_size], new_batch[batch_size:]]
                          for subbatch in subbatches:
                              if not subbatch:
                                  continue
                              worker_future = dask_client.submit(
                                  run_on_batch,
                                  dataset_fs.getsyspath("/"),
                                  subbatch,
                                  pure=False)
                              futures[worker_future] = subbatch
                          del futures[future]

                          # Restart the for loop so that the counter can update
                          break
                else:
                    # Completed successfully - break out of the while loop.
                    break

            # Move the OBJs to the output FS
            print("Copying OBJs to output FS...")
            usd_glob = [x.path for x in dataset_fs.glob("objects/*/*/fillable.obj")]
            for item in tqdm.tqdm(usd_glob):
                out_fs.makedirs(fs.path.dirname(item))
                fs.copy.copy_file(dataset_fs, item, out_fs, item)

            print("Done processing. Archiving things now.")

        # Save the logs
        with pipeline_fs.pipeline_output().open("generate_fillable_volumes.json", "w") as f:
            json.dump({
                "success": len(failed_objects) == 0,
                "failed_objects": sorted(failed_objects),
                "logs": logs,
            }, f)

if __name__ == "__main__":
    main()
