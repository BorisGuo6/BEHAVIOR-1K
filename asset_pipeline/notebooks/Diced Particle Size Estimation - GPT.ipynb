{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe70cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bddl.knowledge_base import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2673260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load particle params and delete everything that shows up there.\n",
    "import json\n",
    "particle_size_option = {}\n",
    "with open(r\"D:/ig_pipeline/metadata/diced_particle_systems_colors.json\") as f:\n",
    "    for particle, infos in json.load(f).items():\n",
    "        particle_size_option[particle] = infos[\"option\"].split(\" \")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da59429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "raw_prompt = \"\"\"\n",
    "You are a helpful assistant that generates particle dimension annotations for particles in a dataset of 3D models.\n",
    "\n",
    "These particles all correspond to the little pieces that you would obtain by cutting the object into small cubes with a knife, i.e. dicing.\n",
    "In everyday life, these particles are often used in cooking or as ingredients in various dishes.\n",
    "They are typically small, uniform pieces that can be easily mixed or cooked with other ingredients.\n",
    "\n",
    "We will represent each particle as a 3D cube with a single parameter, a size, which is the length of one side of the cube in centimeters.\n",
    "This needs to be different for each particle type, since in real life we typically don't dice e.g. onions to the same size as steak. We\n",
    "will provide you with the dictionary definition of the particle system if we have one. We will also provide you with a human annotator's\n",
    "guess as to whether the particle system is a \"fine\" one like for onions or a \"coarse\" one for steak.\n",
    "\n",
    "Give me the estimated particle size of the below particle system in JSON format. The output should be a JSON\n",
    "dictionary containing one key \"size\", whose value is the particle size in centimeters. Output only the JSON - no additional conversation.\n",
    "\"\"\"\n",
    "\n",
    "valid_chars = set(string.ascii_lowercase) | {\" \"}\n",
    "def valid_name(s):\n",
    "    return True\n",
    "    return all(c in valid_chars for c in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d8a221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "OPENAI_API_TOKEN = \"\"   # getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b235b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE_OLLAMA, model_id\n",
    "MODELS = {\n",
    "  # \"gemma3:27b\": (True, None),\n",
    "  # \"llama4:scout\": (True, None),\n",
    "  \"google/gemini-2.5-flash-preview\": (False, None),\n",
    "  \"openai/gpt-4o-mini\": (False, None),\n",
    "  \"meta-llama/llama-4-maverick\": (False, None), # [\"lambda\", \"deepinfra\", \"novita\"]),\n",
    "  \"qwen/qwen3-235b-a22b\": (False, None), # [\"deepinfra\"]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c6b1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "from ollama import AsyncClient\n",
    "from asynciolimiter import Limiter\n",
    "\n",
    "# Limit to 2 requests per second\n",
    "rate_limiter = Limiter(1)\n",
    "\n",
    "import json\n",
    "\n",
    "def strip_code_tags(msg):\n",
    "    if msg.startswith(\"```json\"):\n",
    "        msg = msg[8:]\n",
    "    if msg.startswith(\"```\"):\n",
    "        msg = msg[3:]\n",
    "    if msg.endswith(\"```\"):\n",
    "        msg = msg[:-3]\n",
    "    return msg.strip()\n",
    "\n",
    "ollama_client = AsyncClient()\n",
    "async def query_model_ollama(model_id, obj_name, message, images=None):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": raw_prompt},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    if images is not None:\n",
    "        messages[1][\"images\"] = images\n",
    "    response = await ollama_client.chat(\n",
    "        model=model_id,\n",
    "        messages=messages,\n",
    "    )\n",
    "    response_message = response.message.content\n",
    "    return obj_name, json.loads(strip_code_tags(response_message))\n",
    "\n",
    "openai_client = AsyncOpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=OPENAI_API_TOKEN\n",
    ")\n",
    "async def query_model_openai(model_id, obj_name, message, images=None, providers=None):\n",
    "    await rate_limiter.wait()\n",
    "    content = [\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": message,\n",
    "        }\n",
    "    ]\n",
    "    if images is not None:\n",
    "        for base64_image in images:\n",
    "            data_url = f\"data:image/jpeg;base64,{base64_image}\"\n",
    "            content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": data_url\n",
    "                }\n",
    "            })\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": raw_prompt},\n",
    "        {\"role\": \"user\", \"content\": content},\n",
    "    ]\n",
    "    response = await openai_client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        # response_format={\n",
    "        #     \"type\": \"json_schema\",\n",
    "        #     \"json_schema\": {\n",
    "        #         \"name\": \"object_mass\",\n",
    "        #         \"strict\": True,\n",
    "        #         \"schema\": {\n",
    "        #             \"type\": \"object\",\n",
    "        #             \"properties\": {\n",
    "        #                 \"mass\": {\n",
    "        #                     \"type\": \"number\",\n",
    "        #                     \"description\": \"Mass of object category in kilograms\"\n",
    "        #                 },\n",
    "        #             },\n",
    "        #             \"required\": [\"mass\"],\n",
    "        #             \"additionalProperties\": False\n",
    "        #         }\n",
    "        #     }\n",
    "        # },\n",
    "        messages=messages,\n",
    "        extra_body={\"provider\": {\"only\": providers}} if providers is not None else None,\n",
    "    )\n",
    "    # print(response)\n",
    "    response_message = response.choices[0].message.content\n",
    "    return obj_name, json.loads(strip_code_tags(response_message))\n",
    "\n",
    "async def query_model(model_id, obj_name, message, images=None):\n",
    "    use_ollama, providers = MODELS[model_id]\n",
    "    if use_ollama:\n",
    "        return await query_model_ollama(model_id, obj_name, message, images=images)\n",
    "    else:\n",
    "        return await query_model_openai(model_id, obj_name, message, images=images, providers=providers)\n",
    "\n",
    "async def query_models(obj_name, message, images=None):\n",
    "    tasks = []\n",
    "    model_ids = []\n",
    "    for use_ollama, model_id in MODELS:\n",
    "        model_ids.append(model_id)\n",
    "        if use_ollama:\n",
    "            tasks.append(query_model_ollama(model_id, message, images))\n",
    "        else:\n",
    "            tasks.append(query_model_openai(model_id, message, images))\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return obj_name, {model_id: result[1] for model_id, result in zip(model_ids, results)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae8a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import numpy as np\n",
    "import tqdm.notebook as tqdm\n",
    "import pathlib\n",
    "import asyncio\n",
    "\n",
    "RESULT_DIR = pathlib.Path(r\"D:/ig_pipeline/metadata/vlm_diced_size_predictions/\")\n",
    "\n",
    "async def run_pass(model_id):\n",
    "    model_suffix = model_id.split(\"/\")[-1]\n",
    "    result_path = RESULT_DIR / f\"{model_suffix}.json\"\n",
    "\n",
    "    results = {}\n",
    "    if result_path.exists():\n",
    "        with open(result_path, \"r\") as f:\n",
    "            results = json.load(f)\n",
    "\n",
    "    gpt_missing = [c for c in particle_size_option.keys() if c not in results or results[c] is None]\n",
    "    print(len(gpt_missing))\n",
    "    gpt_missing = gpt_missing\n",
    "\n",
    "    async_futures = []\n",
    "    for ps_name in gpt_missing:\n",
    "        message = \"\"\n",
    "        ps = ParticleSystem.get(ps_name)\n",
    "        diced_synset = ps.synset\n",
    "        assert diced_synset is not None\n",
    "\n",
    "        # Find the non-diced synset\n",
    "        synsets = [s for s in diced_synset.derivative_ancestors if not s.is_derivative]\n",
    "        if not synsets:\n",
    "            print(f\"No non-diced synset found for {ps_name}.\")\n",
    "        synset = synsets[0] if synsets else None\n",
    "\n",
    "        if synset:\n",
    "            if synset.definition:\n",
    "                message += f'\"{ps_name}\": the result of dicing \"{synset.name}\", defined as \"{synset.definition}\"\\n'\n",
    "            else:\n",
    "                parent_synset = synset.parents[0]\n",
    "                assert parent_synset.definition\n",
    "                message += f'\"{ps_name}\": the result of dicing \"{synset.name}\", which is a child of {parent_synset.name}, defined as \"{parent_synset.definition}\"\\n'\n",
    "        else:\n",
    "            message += f'\"{ps_name}\": the result of dicing \"{ps_name}\"\\n'\n",
    "\n",
    "        message += \"Human annotator's guess on fine/coarse: \" + particle_size_option[ps_name] + \"\\n\"\n",
    "            \n",
    "        async_futures.append(query_model(model_id, ps_name, message))\n",
    "\n",
    "    for result in tqdm.tqdm(asyncio.as_completed(async_futures), total=len(async_futures)):\n",
    "        try:\n",
    "            ps_name, result_dict = await result\n",
    "            # filtered_results = {k: v for k, v in result_dict.items() if k in missing_names}\n",
    "            results[ps_name] = result_dict[\"size\"] if \"size\" in result_dict else None\n",
    "\n",
    "            # Save after every result\n",
    "            with open(result_path, \"w\") as f:\n",
    "                json.dump(results, f)\n",
    "        except:\n",
    "            print(\"Error in system\", ps_name)\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39903529",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODELS.keys():\n",
    "    print(\"Running pass for model\", model)\n",
    "    await run_pass(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca6a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "results = defaultdict(dict)\n",
    "for model in MODELS:\n",
    "    model_suffix = model.split(\"/\")[-1]\n",
    "    result_path = RESULT_DIR / f\"{model_suffix}.json\"\n",
    "    if not result_path.exists():\n",
    "        continue\n",
    "    with open(result_path, \"r\") as f:\n",
    "        for cat, mass in json.load(f).items():\n",
    "            results[cat][model] = mass\n",
    "\n",
    "print(len(results))\n",
    "# print(\"Missing categories\", len([c for c in Category.all_objects() if c.name not in results]))\n",
    "# cat_names = {c.name for c in Category.all_objects()}\n",
    "# print(\"Extra categories\", len([x for x in results.keys() if x not in cat_names]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a1e200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the max/min ratio for each category\n",
    "ratios = {}\n",
    "median_sizes = {}\n",
    "for cat, masses in results.items():\n",
    "    assert len(masses) == len(MODELS)\n",
    "    min_mass = min([m for m in masses.values() if m is not None])\n",
    "    max_mass = max([m for m in masses.values() if m is not None])\n",
    "    ratio = max_mass / min_mass\n",
    "    ratios[cat] = ratio\n",
    "    median_sizes[cat] = np.clip(np.median(list(masses.values())), 0.5, 3.)\n",
    "ratios = sorted(ratios.items(), key=lambda x: x[1])\n",
    "print(ratios[:10], ratios[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9d04cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the smallest and largest median sizes\n",
    "smallest = sorted(median_sizes.items(), key=lambda x: x[1])[:10]\n",
    "largest = sorted(median_sizes.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(\"Smallest systems:\")\n",
    "for cat, masses in smallest:\n",
    "    print(cat, masses)\n",
    "\n",
    "print(\"Largest systems:\")\n",
    "for cat, masses in largest:\n",
    "    print(cat, masses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96514005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many are smaller than 1cm?\n",
    "smallest_1cm = [cat for cat, size in median_sizes.items() if size < 1.0]\n",
    "print(\"Number of systems with median size < 1cm:\", len(smallest_1cm))\n",
    "print(\"Examples:\", smallest_1cm[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee86356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "with open(r\"D:/ig_pipeline/metadata/diced_particle_systems_colors.json\") as f:\n",
    "    saved_data = json.load(f)\n",
    "for particle, infos in saved_data.items():\n",
    "    infos[\"llm_size_cm\"] = median_sizes[particle]\n",
    "with open(r\"D:/ig_pipeline/metadata/diced_particle_systems_colors.json\", \"w\") as f:\n",
    "    json.dump(saved_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876b5bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
