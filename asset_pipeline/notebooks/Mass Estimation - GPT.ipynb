{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe70cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bddl.knowledge_base import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2673260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load particle params and delete everything that shows up there.\n",
    "import csv, json\n",
    "synset_has_particle_density = set()\n",
    "with open(\"/scr/BEHAVIOR-1K/asset_pipeline/metadata/substance_hyperparams.csv\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        synset = Synset.get(row[\"synset\"].strip())\n",
    "        if synset is None:\n",
    "            print(f\"Synset {row['synset']} not found\")\n",
    "            continue\n",
    "        has_density = \"particle_density\" in json.loads(row[\"hyperparams\"])\n",
    "        if has_density:\n",
    "            synset_has_particle_density.add(synset)\n",
    "        elif \"visualSubstance\" not in synset.property_names:\n",
    "            print(f\"Synset {synset} does not have particle density\")\n",
    "\n",
    "synset_is_visual = {s for s in Synset.all_objects() if \"visualSubstance\" in s.property_names}\n",
    "ignore_synsets = synset_has_particle_density | synset_is_visual\n",
    "ignore_categories = {c for s in ignore_synsets for c in s.categories}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da59429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "raw_prompt = \"\"\"\n",
    "You are a helpful assistant that generates average mass annotations for objects in a dataset of 3D models.\n",
    "Give me the estimated average mass of the below category of objects in JSON format. The output should be a JSON\n",
    "dictionary containing one key \"mass\", whose value is the category mass in kilograms. Output only the JSON - no additional conversation.\n",
    "\n",
    "* When you have objects that are named like x_box or x_bottle (e.g. wine_bottle, rice_bag) that are suffixed with a container name, assume that those are empty containers.\n",
    "* Objects that are prefixed with a container type like bottle_of_x or box_of_x (e.g. bottle_of_wine, bag_of_rice) should be assumed to be full.\n",
    "* When you have a particle based object (e.g. rice, flour), assume that it is a single particle of that object.\n",
    "* If you see \"atomizer\", assume that it is a spray bottle containing that liquid (and is full).\n",
    "* If you see cooked_ in the name, assume that it is a cooked version of that object (e.g. cooked_rice, cooked_pasta) and has roughly the same mass as the raw version.\n",
    "* If you see \"half\" in the name, assume that it is the object that you would obtain by cutting the object once with a knife (e.g. half_banana, half_apple).\n",
    "* If you see \"diced\" in the name, assume that it is one of each of the pieces that you would obtain by cutting the object into small cubes with a knife (e.g. diced_banana, diced_apple).\n",
    "* If I give you images, assume that they are some example photos of some objects that belong to the category.\n",
    "\"\"\"\n",
    "\n",
    "valid_chars = set(string.ascii_lowercase) | {\" \"}\n",
    "def valid_name(s):\n",
    "    return True\n",
    "    return all(c in valid_chars for c in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d8a221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "OPENAI_API_TOKEN = \"\"   # getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b235b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE_OLLAMA, model_id\n",
    "MODELS = {\n",
    "  # \"gemma3:27b\": (True, None),\n",
    "  # \"llama4:scout\": (True, None),\n",
    "  \"google/gemini-2.5-flash-preview\": (False, None),\n",
    "  \"openai/gpt-4o-mini\": (False, None),\n",
    "  \"meta-llama/llama-4-maverick\": (False, [\"lambda\", \"deepinfra\", \"novita\"]),\n",
    "  \"qwen/qwen2.5-vl-72b-instruct\": (False, [\"nebius\"]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c6b1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "from ollama import AsyncClient\n",
    "from asynciolimiter import Limiter\n",
    "\n",
    "# Limit to 2 requests per second\n",
    "rate_limiter = Limiter(1)\n",
    "\n",
    "import json\n",
    "\n",
    "def strip_code_tags(msg):\n",
    "    if msg.startswith(\"```json\"):\n",
    "        msg = msg[8:]\n",
    "    if msg.startswith(\"```\"):\n",
    "        msg = msg[3:]\n",
    "    if msg.endswith(\"```\"):\n",
    "        msg = msg[:-3]\n",
    "    return msg.strip()\n",
    "\n",
    "ollama_client = AsyncClient()\n",
    "async def query_model_ollama(model_id, obj_name, message, images=None):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": raw_prompt},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    if images is not None:\n",
    "        messages[1][\"images\"] = images\n",
    "    response = await ollama_client.chat(\n",
    "        model=model_id,\n",
    "        messages=messages,\n",
    "    )\n",
    "    response_message = response.message.content\n",
    "    return obj_name, json.loads(strip_code_tags(response_message))\n",
    "\n",
    "openai_client = AsyncOpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=OPENAI_API_TOKEN\n",
    ")\n",
    "async def query_model_openai(model_id, obj_name, message, images=None, providers=None):\n",
    "    await rate_limiter.wait()\n",
    "    content = [\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": message,\n",
    "        }\n",
    "    ]\n",
    "    if images is not None:\n",
    "        for base64_image in images:\n",
    "            data_url = f\"data:image/jpeg;base64,{base64_image}\"\n",
    "            content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": data_url\n",
    "                }\n",
    "            })\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": raw_prompt},\n",
    "        {\"role\": \"user\", \"content\": content},\n",
    "    ]\n",
    "    response = await openai_client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        # response_format={\n",
    "        #     \"type\": \"json_schema\",\n",
    "        #     \"json_schema\": {\n",
    "        #         \"name\": \"object_mass\",\n",
    "        #         \"strict\": True,\n",
    "        #         \"schema\": {\n",
    "        #             \"type\": \"object\",\n",
    "        #             \"properties\": {\n",
    "        #                 \"mass\": {\n",
    "        #                     \"type\": \"number\",\n",
    "        #                     \"description\": \"Mass of object category in kilograms\"\n",
    "        #                 },\n",
    "        #             },\n",
    "        #             \"required\": [\"mass\"],\n",
    "        #             \"additionalProperties\": False\n",
    "        #         }\n",
    "        #     }\n",
    "        # },\n",
    "        messages=messages,\n",
    "        extra_body={\"provider\": {\"only\": providers}} if providers is not None else None,\n",
    "    )\n",
    "    # print(response)\n",
    "    response_message = response.choices[0].message.content\n",
    "    return obj_name, json.loads(strip_code_tags(response_message))\n",
    "\n",
    "async def query_model(model_id, obj_name, message, images=None):\n",
    "    use_ollama, providers = MODELS[model_id]\n",
    "    if use_ollama:\n",
    "        return await query_model_ollama(model_id, obj_name, message, images=images)\n",
    "    else:\n",
    "        return await query_model_openai(model_id, obj_name, message, images=images, providers=providers)\n",
    "\n",
    "async def query_models(obj_name, message, images=None):\n",
    "    tasks = []\n",
    "    model_ids = []\n",
    "    for use_ollama, model_id in MODELS:\n",
    "        model_ids.append(model_id)\n",
    "        if use_ollama:\n",
    "            tasks.append(query_model_ollama(model_id, message, images))\n",
    "        else:\n",
    "            tasks.append(query_model_openai(model_id, message, images))\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return obj_name, {model_id: result[1] for model_id, result in zip(model_ids, results)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f64355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import cv2\n",
    "import base64\n",
    "import random\n",
    "import pathlib\n",
    "\n",
    "def sample_frames_from_image(filename, n):\n",
    "    cap = cv2.VideoCapture(filename)\n",
    "    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT) / 2  # only from the first half\n",
    "    \n",
    "    # Randomly pick a frame and make the other ones be equally spaced from it\n",
    "    start_frame = random.randint(0, int(total_frames))\n",
    "    frame_indices = sorted([int(start_frame + i * (total_frames / n)) % total_frames for i in range(n)])\n",
    "\n",
    "    frames = []\n",
    "    for i in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ret, frame = cap.read()\n",
    "        jpeg_image = cv2.imencode('.jpg', frame)[1].tobytes()\n",
    "        frames.append(base64.b64encode(jpeg_image).decode('utf-8'))\n",
    "    cap.release()\n",
    "\n",
    "    return frames\n",
    "\n",
    "NUM_IMAGES = 4\n",
    "def sample_category_images(category):\n",
    "    objects = list(category.objects)\n",
    "    objects_videos = [pathlib.Path(f\"/scr/BEHAVIOR-1K/asset_pipeline/artifacts/pipeline/object_images/{obj.name}.mp4\") for obj in objects]\n",
    "    objects_videos = [obj for obj in objects_videos if obj.exists()]\n",
    "    if len(objects_videos) == 0:\n",
    "        return None\n",
    "    objects_videos = collections.Counter(random.choices(objects_videos, k=NUM_IMAGES))\n",
    "    objects_images = [img for obj, cnt in objects_videos.items() for img in sample_frames_from_image(obj, cnt)]\n",
    "    random.shuffle(objects_images)\n",
    "    return objects_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ebbd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from base64 import b64decode\n",
    "\n",
    "display.Image(b64decode(sample_category_images(Category.get(\"walls\"))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae8a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import numpy as np\n",
    "import tqdm.notebook as tqdm\n",
    "import pathlib\n",
    "import asyncio\n",
    "\n",
    "RESULT_DIR = pathlib.Path(\"/scr/BEHAVIOR-1K/asset_pipeline/metadata/vlm_mass_predictions/\")\n",
    "\n",
    "def human_readable_size(size):\n",
    "    \"\"\"Convert a size in meters to a human-readable string with a unit of mm, cm, or m.\"\"\"\n",
    "    if size < 0.001:\n",
    "        return f\"{size * 1e3:.2f} mm\"\n",
    "    elif size < 1:\n",
    "        return f\"{size * 1e2:.2f} cm\"\n",
    "    else:\n",
    "        return f\"{size:.2f} m\"\n",
    "\n",
    "async def run_pass(model_id):\n",
    "    model_suffix = model_id.split(\"/\")[-1]\n",
    "    result_path = RESULT_DIR / f\"{model_suffix}.json\"\n",
    "\n",
    "    results = {}\n",
    "    if result_path.exists():\n",
    "        with open(result_path, \"r\") as f:\n",
    "            results = json.load(f)\n",
    "\n",
    "    gpt_missing = [c for c in Category.all_objects() if c.name not in results and len(c.objects) > 0 and c not in ignore_categories]\n",
    "    print(len(gpt_missing))\n",
    "    # gpt_missing = gpt_missing[:100]\n",
    "\n",
    "    async_futures = []\n",
    "    for c in gpt_missing:\n",
    "        message = \"\"\n",
    "        images = []\n",
    "        cat = c.name\n",
    "        synset = c.synset\n",
    "        assert c.synset is not None\n",
    "\n",
    "        if c.synset.definition:\n",
    "            message += f'\"{cat}\": a kind of \"{synset.name}\", defined as \"{synset.definition}\"\\n'\n",
    "        else:\n",
    "            parent_synset = synset.parents[0]\n",
    "            assert parent_synset.definition\n",
    "            message += f'\"{cat}\": a kind of \"{synset.name}\", which is a child of {parent_synset.name}, defined as \"{parent_synset.definition}\"\\n'\n",
    "\n",
    "        avg_dim = np.array([list(obj.bounding_box_size) for obj in c.objects if obj.bounding_box_size is not None]).mean(axis=0)\n",
    "        avg_dim_readable = \", \".join([human_readable_size(d) for d in avg_dim])\n",
    "        message += \"Average dimensions: \" + avg_dim_readable + \"\\n\"\n",
    "        # print(message)\n",
    "        c_images = sample_category_images(c)\n",
    "        if c_images is not None:\n",
    "            images.extend(c_images)\n",
    "            \n",
    "        async_futures.append(query_model(model_id, cat, message, images))\n",
    "\n",
    "    for result in tqdm.tqdm(asyncio.as_completed(async_futures), total=len(async_futures)):\n",
    "        try:\n",
    "            cat, result_dict = await result\n",
    "            # filtered_results = {k: v for k, v in result_dict.items() if k in missing_names}\n",
    "            results[cat] = result_dict[\"mass\"] if \"mass\" in result_dict else None\n",
    "\n",
    "            # Save after every result\n",
    "            with open(result_path, \"w\") as f:\n",
    "                json.dump(results, f)\n",
    "        except:\n",
    "            print(\"Error in cat\", cat)\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39903529",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODELS:\n",
    "    print(\"Running pass for model\", model)\n",
    "    await run_pass(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca6a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "results = defaultdict(dict)\n",
    "for model in MODELS:\n",
    "    model_suffix = model.split(\"/\")[-1]\n",
    "    result_path = RESULT_DIR / f\"{model_suffix}.json\"\n",
    "    if not result_path.exists():\n",
    "        continue\n",
    "    with open(result_path, \"r\") as f:\n",
    "        for cat, mass in json.load(f).items():\n",
    "            results[cat][model] = mass\n",
    "\n",
    "print(results)\n",
    "# print(\"Missing categories\", len([c for c in Category.all_objects() if c.name not in results]))\n",
    "# cat_names = {c.name for c in Category.all_objects()}\n",
    "# print(\"Extra categories\", len([x for x in results.keys() if x not in cat_names]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a1e200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the max/min ratio for each category\n",
    "ratios = {}\n",
    "for cat, masses in results.items():\n",
    "    assert len(masses) == len(MODELS)\n",
    "    min_mass = min([m for m in masses.values() if m is not None])\n",
    "    max_mass = max([m for m in masses.values() if m is not None])\n",
    "    ratio = max_mass / min_mass\n",
    "    ratios[cat] = ratio\n",
    "ratios = sorted(ratios.items(), key=lambda x: x[1])\n",
    "print(ratios[:10], ratios[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629df332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now print everything in the order they show up in the category sheet so that we can paste this there.\n",
    "import csv\n",
    "with open(\"/scr/BEHAVIOR-1K/asset_pipeline/metadata/category_mapping.csv\", \"r\") as f:\n",
    "    category_list = list(csv.DictReader(f))\n",
    "\n",
    "model_keys = sorted(MODELS.keys())\n",
    "print(\",\".join(model_keys))\n",
    "for cat in category_list:\n",
    "    name = cat[\"category\"]\n",
    "    values = [str(results[name][model]) for model in model_keys] if name in results else [\"\"] * len(model_keys)\n",
    "    print(\",\".join(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff611f31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnigibson45",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
